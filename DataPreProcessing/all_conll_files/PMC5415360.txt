ENFORCING O
CO-EXPRESSION O
IN O
MULTIMODAL O
REGRESSION O
FRAMEWORK O
We O
consider O
the O
problem O
of O
multimodal O
data O
integration O
for O
the O
study O
of O
complex O
neurological O
diseases O
( O
e.g O
. O

schizophrenia O
) O
. O

Among O
the O
challenges O
arising O
in O
such O
situation O
, O
estimating O
the O
link O
between O
genetic O
and O
neurological O
variability O
within O
a O
population O
sample O
has O
been O
a O
promising O
direction O
. O

A O
wide O
variety O
of O
statistical O
models O
arose O
from O
such O
applications O
. O

For O
example O
, O
Lasso O
regression O
and O
its O
multitask O
extension O
are O
often O
used O
to O
fit O
a O
multivariate O
linear O
relationship O
between O
given O
phenotype O
( O
s O
) O
and O
associated O
observations O
. O

Other O
approaches O
, O
such O
as O
canonical O
correlation O
analysis O
( O
CCA O
) O
, O
are O
widely O
used O
to O
extract O
relationships O
between O
sets O
of O
variables O
from O
different O
modalities O
. O

In O
this O
paper O
, O
we O
propose O
an O
exploratory O
multivariate O
method O
combining O
these O
two O
methods O
. O

More O
Specifically O
, O
we O
rely O
on O
a O
’ O
CCA-type O
’ O
formulation O
in O
order O
to O
regularize O
the O
classical O
multimodal O
Lasso O
regression O
problem O
. O

The O
underlying O
motivation O
is O
to O
extract O
discriminative O
variables O
that O
display O
are O
also O
co-expressed O
across O
modalities O
. O

We O
first O
evaluate O
the O
method O
on O
a O
simulated O
dataset O
, O
and O
further O
validate O
it O
using O
Single O
Nucleotide O
Polymorphisms O
( O
SNP O
) O
and O
functional O
Magnetic O
Resonance O
Imaging O
( O
fMRI O
) O
data O
for O
the O
study O
of O
schizophrenia O
. O

2 O
. O

Methods O
2.1 O
. O

Learning O
with O
L1 O
penalty O
We O
consider O
M O
∈ O
ℕ+ O
distinct O
( O
i.e O
. O

from O
different O
modalities O
) O
datasets O
with O
n O
samples O
and O
pm O
∈ O
ℕ+ O
( O
m O
= O
1 O
, O
‥ O
, O
M O
) O
variables O
each O
. O

The O
m-th O
dataset O
is O
represented O
by O
a O
matrix O
Xm O
∈ O
ℝn×pm O
. O

Additionally O
, O
each O
sample O
is O
assigned O
a O
class O
label O
( O
e.g O
. O

case/controls O
) O
yi O
∈ O
{ O
−1 O
, O
1 O
} O
, O
i O
= O
1 O
, O
‥ O
, O
n. O
Our O
goal O
is O
to O
look O
for O
a O
linear O
link O
between O
those O
class O
labels O
and O
the O
M O
data O
matrices O
. O

Let O
us O
consider O
the O
following O
regression O
model O
: O
The O
model O
described O
by O
Eq O
. O

1 O
performs O
both O
variable O
selection O
and O
regularization O
. O

It O
often O
improves O
the O
prediction O
accuracy O
and O
interpretability O
of O
the O
results O
compared O
to O
the O
use O
of O
classical O
ℓ2 O
norm O
regularization O
terms O
, O
especially O
when O
the O
number O
of O
variables O
is O
far O
greater O
than O
the O
number O
of O
observations O
. O

In O
some O
situations O
, O
we O
have O
several O
output O
vectors O
ym O
, O
∀m O
= O
1 O
, O
‥ O
, O
M O
and O
the O
m O
datasets O
are O
from O
the O
same O
modality O
: O
multi-task O
Lasso O
was O
proposed O
to O
capture O
shared O
structures O
among O
the O
various O
regression O
vectors O
. O

We O
consider O
the O
following O
model O
: O
where O
P O
is O
the O
dimension O
of O
the O
problem O
and O
βp O
is O
the O
p-th O
row O
of O
the O
matrix O
such O
that O
β O
= O
[ O
β1 O
, O
‥ O
, O
βm O
] O
( O
i.e O
. O

the O
βm O
are O
stacked O
horizontally O
) O
. O

Such O
norm O
is O
also O
referred O
to O
as O
the O
ℓ1/ℓ2 O
norm O
, O
and O
is O
used O
to O
both O
enforce O
joint O
sparsity O
across O
the O
multiple O
βm O
and O
estimate O
only O
a O
few O
non-zero O
coefficients O
. O

Enforcing O
regularity O
within O
a O
modality O
( O
and O
across O
tasks O
) O
has O
been O
an O
active O
aspect O
of O
regression O
models O
, O
and O
has O
proven O
to O
increase O
reliability O
and O
results O
. O

However O
, O
since O
often O
pair-wise O
closeness O
is O
looked O
for O
in O
the O
common O
subspace O
, O
such O
methods O
will O
often O
fail O
to O
capture O
relationships O
across O
modalities O
. O

2.2 O
. O

Collaborative O
learning O
Collaborative O
( O
or O
Co-regularized O
) O
methods O
are O
based O
on O
the O
optimization O
of O
measures O
of O
agreement O
and O
smoothness O
across O
multi-modal O
datasets O
. O

Smoothness O
across O
modalities O
is O
enforced O
through O
a O
joint O
regularization O
term O
. O

Their O
general O
model O
can O
be O
expressed O
as O
follows O
: O
where O
the O
Um O
, O
m O
= O
1 O
, O
‥ O
, O
M O
are O
arbitrary O
matrices O
whose O
roles O
are O
to O
control O
the O
cross-view O
joint O
regularization O
between O
each O
pair O
of O
vectors O
( O
βm O
, O
βq O
) O
, O
m O
, O
q O
= O
1 O
, O
‥ O
, O
M. O
Scalar O
parameter O
γ O
≥ O
0 O
controls O
the O
influence O
of O
such O
cross-regularization O
term O
. O

Notice O
that O
if O
γ O
= O
0 O
, O
we O
fall O
back O
on O
the O
original O
Lasso O
formulation O
. O

Collaborative O
learning O
is O
an O
interesting O
extension O
of O
Eq.1 O
allowing O
the O
user O
to O
explicitly O
enforce O
regularization O
across O
modalities O
. O

In O
this O
work O
, O
we O
rely O
on O
a O
special O
case O
of O
collaborative O
methods O
( O
introduced O
later O
in O
section O
3 O
) O
to O
address O
the O
following O
aspects O
: O
( O
i O
) O
Extend O
the O
regularization O
idea O
across O
modalities O
, O
( O
ii O
) O
Assume O
that O
relationships O
between O
variable O
are O
not O
available O
as O
a O
prior O
knowledge O
( O
as O
opposed O
, O
e.g. O
, O
to O
Xin O
) O
, O
( O
iii O
) O
Define O
links O
between O
components O
using O
correlation O
measure O
. O

To O
do O
so O
, O
we O
first O
briefly O
introduce O
in O
the O
next O
section O
some O
of O
the O
classical O
methods O
to O
extract O
meaningful O
relationships O
between O
variables O
across O
modalities O
. O

2.3 O
. O

Extracting O
relationship O
between O
datasets O
A O
wide O
variety O
of O
problems O
amount O
to O
the O
joint O
analysis O
of O
multimodal O
datasets O
describing O
the O
same O
set O
of O
observations O
. O

Often O
, O
a O
mean O
to O
perform O
such O
analysis O
is O
to O
learn O
projection O
subspaces O
using O
paired O
samples O
such O
that O
structures O
of O
interest O
appear O
more O
clearly O
. O

Some O
of O
these O
methods O
are O
for O
example O
: O
Canonical O
correlation O
analysis O
( O
CCA O
) O
, O
Partial O
least O
squares O
( O
PLS O
) O
or O
cross-modal O
factor O
analysis O
( O
CFA O
) O
. O

Among O
them O
, O
CCA O
is O
probably O
the O
most O
widely O
used O
. O

Its O
goal O
is O
to O
extract O
linear O
combinations O
of O
variables O
with O
maximal O
correlation O
between O
two O
( O
or O
more O
) O
datasets O
. O

Using O
similar O
notations O
as O
in O
the O
previous O
section O
, O
and O
assuming O
M O
= O
2 O
, O
one O
formulation O
of O
CCA O
is O
expressed O
as O
follow O
: O
to O
which O
a O
constraint O
on O
the O
norm O
of O
canonical O
vectors O
β1 O
, O
β2 O
is O
added O
to O
avoid O
the O
trivial O
null O
solution O
. O

In O
recent O
years O
, O
CCA O
has O
been O
widely O
applied O
to O
genomic O
data O
analysis O
. O

As O
a O
consequence O
, O
many O
studies O
on O
sparse O
versions O
of O
CCA O
( O
sCCA O
) O
have O
been O
proposed O
to O
cope O
with O
the O
high O
dimension O
but O
low O
sample O
size O
problem O
. O

In O
the O
next O
section O
, O
we O
will O
rely O
on O
a O
CCA O
term O
to O
measure O
co-expression O
between O
variables O
from O
different O
modalities O
. O

3 O
. O

Enforcing O
cross-correlation O
in O
regression O
problems O
3.1 O
. O

MT-CoReg O
formulation O
As O
discussed O
in O
Section O
1 O
, O
several O
methods O
have O
been O
proposed O
to O
: O
( O
i O
) O
Associate O
a O
phenotype O
and O
datasets O
while O
enforcing O
prior O
over O
solution O
, O
( O
ii O
) O
Extract O
relationships O
between O
coupled O
or O
co-expressed O
datasets O
. O

In O
the O
present O
study O
, O
we O
propose O
to O
associate O
both O
the O
regression O
and O
CCA O
frameworks O
in O
the O
case O
of O
M O
= O
2 O
datasets O
. O

Our O
motivation O
is O
to O
extract O
informative O
features O
that O
also O
display O
a O
significant O
amount O
of O
correlation O
across O
modalities O
. O

A O
simple O
way O
to O
combine O
Lasso O
and O
sparse O
CCA O
would O
be O
a O
weighted O
combination O
of O
Eq O
. O

( O
1 O
) O
and O
Eq O
. O

( O
4 O
) O
: O
where O
γ O
∈ O
[ O
0 O
, O
1 O
] O
is O
a O
weight O
parameter O
. O

Notice O
that O
Eq O
. O

( O
5 O
) O
can O
be O
expressed O
within O
the O
collaborative O
framework O
introduced O
in O
Section O
2.2 O
. O

If O
we O
take O
a O
look O
at O
Eq O
. O

( O
3 O
) O
with O
M O
= O
2 O
, O
U1 O
= O
X1 O
and O
U2 O
= O
X2 O
, O
we O
fall O
back O
on O
Eq O
. O
( O
5 O
) O
. O

Let O
us O
call O
this O
model O
CoReg O
for O
Collaborative O
Regression O
. O

Interestingly O
, O
a O
similar O
model O
has O
been O
considered O
before O
by O
Gross O
to O
perform O
prediction O
using O
breast O
cancer O
data O
. O

However O
, O
to O
our O
opinion O
, O
such O
formulation O
might O
prove O
to O
be O
too O
constraining O
. O

It O
essentially O
amounts O
to O
force O
each O
component O
of O
the O
βm O
’ O
s O
to O
fit O
both O
the O
regression O
term O
and O
the O
CCA O
one O
. O

We O
illustrate O
such O
behaviour O
using O
a O
toy O
dataset O
later O
in O
Section O
3.4 O
. O

Since O
our O
goal O
is O
to O
perform O
feature O
selection O
, O
we O
may O
allow O
the O
model O
to O
be O
slightly O
more O
flexible O
. O

We O
thus O
propose O
an O
alternative O
formulation O
by O
first O
duplicating O
each O
βm O
into O
two O
components O
such O
that O
: O
where O
αm O
, O
θm O
are O
vectors O
from O
ℝpm O
. O

As O
a O
consequence O
, O
the O
βm O
’ O
s O
are O
now O
matrices O
such O
that O
βm O
∈ O
ℝpm×2 O
∀m O
= O
1 O
, O
2 O
. O

We O
then O
propose O
the O
following O
MT-CoReg O
formulation O
: O
where O
is O
the O
i-th O
row O
of O
βm O
, O
i.e O
. O
. O

The O
third O
term O
of O
Eq O
. O

( O
3.3 O
) O
is O
simply O
the O
ℓ1/ℓ2 O
norm O
of O
each O
of O
the O
βm O
. O

As O
we O
can O
observe O
from O
looking O
at O
Eq O
. O

( O
3.3 O
) O
, O
each O
’ O
component O
’ O
( O
i.e O
. O

column O
of O
βm O
) O
will O
be O
involved O
in O
separate O
parts O
of O
the O
functional O
J O
: O
( O
i O
) O
components O
αm O
are O
the O
fit O
to O
the O
regression O
term O
of O
Eq O
. O

( O
3.3 O
) O
, O
( O
ii O
) O
components O
θm O
are O
the O
fit O
of O
the O
CCA O
term O
of O
Eq O
. O
( O
3.3 O
) O
. O

Each O
pair O
( O
αm O
, O
θm O
) O
and O
m O
= O
1 O
, O
2 O
is O
coupled O
through O
the O
use O
of O
the O
ℓ1/ℓ2 O
norm O
from O
the O
third O
term O
in O
Eq O
. O
( O
3.3 O
) O
. O

Although O
their O
values O
are O
different O
, O
shared O
sparsity O
patterns O
are O
encouraged O
within O
each O
pair O
( O
αm O
, O
θm O
) O
. O

As O
a O
consequence O
, O
we O
allow O
the O
method O
to O
be O
significantly O
more O
flexible O
in O
terms O
of O
solutions O
: O
different O
values O
can O
be O
taken O
to O
simultaneously O
fit O
the O
Regression O
and O
CCA O
parts O
. O

We O
hope O
that O
such O
framework O
will O
encourage O
the O
selection O
of O
features O
that O
are O
discriminative O
( O
via O
the O
regression O
part O
) O
but O
also O
co-expressed O
across O
modalities O
( O
via O
the O
CCA O
part O
) O
. O

Note O
that O
when O
γ O
= O
0 O
, O
criterion O
( O
3.3 O
) O
essentially O
reduces O
to O
the O
initial O
regression O
problem O
of O
Eq O
. O

( O
1 O
) O
, O
while O
setting O
γ O
= O
1 O
amounts O
to O
solving O
a O
conventional O
sparse O
CCA O
problem O
. O

A O
schematic O
view O
of O
the O
MT-CoReg O
pipeline O
can O
be O
seen O
in O
Fig O
. O
( O
1 O
) O
. O

In O
the O
next O
section O
, O
we O
briefly O
explain O
how O
to O
solve O
the O
problem O
described O
in O
Eq O
. O
( O
3.3 O
) O
. O

3.2 O
. O

Optimization O
Initialization O
: O
estimate O
initial O
values O
for O
α1 O
, O
β1 O
, O
α2 O
, O
β2 O
using O
ridge O
regression O
and O
ridge O
CCA O
. O

Assume O
β1 O
’ O
s O
value O
fixed O
, O
and O
update O
β2 O
using O
Eq O
. O
( O
8 O
) O
. O

Assume O
β2 O
’ O
s O
value O
fixed O
, O
and O
update O
β1 O
using O
the O
adapted O
version O
of O
Eq O
. O
( O
8 O
) O
. O

Go O
back O
to O
step O
2. O
until O
convergence O
We O
solve O
the O
problem O
from O
Eq O
. O

( O
3.3 O
) O
by O
optimizing O
the O
βm O
’ O
s O
alternatively O
over O
iterations O
until O
convergence O
, O
in O
a O
similar O
fashion O
to O
Wilms O
et O
al O
. O

formulation O
of O
sCCA O
. O

Suppose O
we O
have O
an O
initial O
value O
for O
β1 O
, O
and O
want O
to O
estimate O
β2 O
. O

Updating O
matrix O
β2 O
can O
be O
recast O
into O
a O
problem O
of O
the O
following O
form O
: O
where O
Obviously O
, O
Eq O
. O

( O
8 O
) O
is O
a O
classical O
group-lasso O
regression O
problem O
( O
cf O
. O

Eq O
. O
( O
2 O
) O
) O
. O

It O
is O
easy O
to O
show O
that O
updating O
β1 O
reduces O
to O
solving O
a O
similar O
problem O
. O

As O
a O
consequence O
, O
solving O
our O
mixed O
Lasso/CCA O
problem O
from O
Eq O
. O

( O
3.3 O
) O
can O
be O
briefly O
summarized O
as O
: O
3.3 O
. O

Parameter O
selection O
Solving O
problem O
from O
Eq O
. O

( O
3.3 O
) O
requires O
the O
estimation O
of O
two O
parameters O
, O
λ O
and O
γ O
, O
which O
respectively O
control O
the O
weights O
of O
the O
sparsity O
and O
the O
co-expression O
regularization O
terms O
. O

The O
choice O
of O
sparsity O
parameter O
λ O
for O
this O
type O
of O
problems O
is O
known O
to O
display O
a O
high O
sensitivity O
. O

In O
order O
to O
make O
the O
searching O
process O
more O
robust O
, O
we O
chose O
to O
let O
the O
sparsity O
level O
of O
the O
solution O
control O
the O
tuning O
parameter O
value O
. O

Consider O
a O
column O
vector O
β O
∈ O
ℝp O
( O
e.g O
. O

a O
column O
of O
β O
from O
Eq O
. O

) O
: O
let O
us O
denote O
|β|κ O
the O
κ-th O
( O
κ O
∈ O
ℕ+ O
) O
largest O
absolute O
magnitude O
of O
λ O
. O

We O
can O
Define O
a O
correspondence O
between O
λ O
and O
κ O
by O
making O
sure O
that O
for O
each O
iteration O
, O
we O
have O
λ O
∈ O
[ O
|β|κ O
, O
|β|κ+1 O
] O
. O

The O
selection O
can O
be O
looked O
for O
around O
the O
sample O
size O
( O
i.e O
. O

κ O
= O
n O
for O
the O
entire O
estimation O
process O
) O
, O
which O
helps O
drastically O
stabilize O
the O
estimation O
process O
in O
practice O
. O

As O
for O
the O
estimation O
of O
γ O
, O
we O
chose O
to O
rely O
on O
a O
technique O
introduced O
by O
Sun O
et O
al O
. O

based O
on O
variable O
selection O
stability O
. O

Its O
main O
goal O
is O
to O
select O
a O
given O
tuning O
parameter O
so O
that O
the O
associated O
variable O
selection O
method O
( O
in O
our O
case O
, O
the O
model O
from O
Eq O
. O

( O
3.3 O
) O
) O
is O
stable O
in O
terms O
of O
the O
features O
it O
selects O
. O

In O
this O
framework O
, O
the O
training O
set O
is O
split O
in O
two O
halves O
using O
resampling O
( O
bootstrap O
resampling O
in O
our O
case O
) O
. O

The O
variable O
selection O
method O
is O
then O
applied O
to O
each O
of O
the O
subsamples O
along O
a O
grid O
of O
candidate O
values O
for O
the O
parameter O
. O

Kappa O
selection O
criterion O
is O
then O
used O
to O
measure O
the O
degree O
of O
agreement O
between O
the O
two O
sets O
of O
variables O
obtained O
for O
a O
given O
parameter O
value O
. O

This O
process O
is O
then O
repeated O
a O
number O
of O
times O
, O
and O
an O
approximated O
measure O
of O
selection O
consistency O
is O
derived O
. O

The O
parameter O
value O
for O
which O
this O
consistency O
is O
the O
highest O
( O
after O
correction O
for O
the O
number O
of O
non-zeros O
elements O
retained O
) O
is O
the O
one O
kept O
for O
the O
estimation O
. O

3.4 O
. O

MT-CoReg O
VS. O
CoReg O
As O
mentioned O
earlier O
in O
Section O
3.1 O
, O
in O
their O
CoReg O
model O
from O
Eq O
. O

( O
5 O
) O
Gross O
et O
al O
. O

did O
not O
separate O
the O
solution O
vectors O
βm O
into O
two O
components O
. O

We O
then O
propose O
to O
illustrate O
the O
behavior O
of O
both O
models O
( O
Eq O
. O

( O
5 O
) O
and O
Eq O
. O

( O
3.3 O
) O
) O
on O
a O
toy O
dataset O
. O

We O
generated O
M O
= O
2 O
data O
matrices O
X1 O
, O
X2 O
such O
that O
p1 O
= O
p2 O
= O
30 O
and O
n O
= O
50 O
observations O
. O

We O
used O
a O
latent O
variable O
model O
to O
simulate O
cross-correlated O
components O
so O
that O
columns O
p O
= O
[ O
1 O
, O
‥5 O
] O
∪ O
[ O
10 O
, O
‥15 O
] O
of O
X1 O
, O
X2 O
are O
mutually O
co-expressed O
. O

We O
further O
use O
columns O
p O
= O
[ O
10 O
, O
‥ O
15 O
] O
∪ O
[ O
20 O
, O
‥25 O
] O
to O
generate O
a O
phenotype O
vector O
y O
such O
that O
yi O
∈ O
{ O
−1 O
; O
1 O
} O
. O

With O
such O
setup O
, O
columns O
p O
= O
[ O
10 O
, O
‥ O
15 O
] O
correspond O
to O
both O
non-zeros O
values O
in O
the O
true O
regression O
and O
canonical O
coefficients O
. O

Furthermore O
, O
let O
us O
point O
out O
that O
these O
non-zero O
values O
are O
diferent O
( O
canonical O
coefficients O
’ O
amplitude O
is O
lower O
than O
the O
regression O
ones O
) O
. O

This O
setup O
can O
be O
seen O
in O
the O
first O
row O
of O
Fig O
. O

( O
2 O
, O
Truth O
) O
, O
where O
the O
blue O
and O
red O
curves O
are O
the O
values O
taken O
by O
the O
canonical O
and O
regression O
coefficients O
respectively O
. O

Resulting O
estimates O
for O
sCCA O
, O
Lasso O
, O
CoReg O
as O
well O
as O
proposed O
method O
MT-CoReg O
can O
also O
be O
seen O
in O
Fig O
. O
( O
2 O
) O
. O

In O
such O
scenario O
, O
while O
CoReg O
model O
assumes O
that O
regression O
and O
canonical O
coefficients O
have O
identical O
values O
, O
MT-CoReg O
has O
a O
wider O
scope O
and O
allows O
a O
finer O
joint O
estimation O
of O
both O
components O
types O
. O

4 O
. O

Experiments O
In O
this O
section O
, O
we O
evaluate O
the O
proposed O
estimator O
from O
Eq.3.3 O
. O

Performances O
will O
be O
assessed O
in O
terms O
of O
feature O
selection O
relevance O
on O
both O
simulated O
and O
real O
data O
. O

4.1 O
. O

Results O
on O
synthetic O
data O
For O
our O
first O
test O
, O
we O
simulate O
both O
fMRI O
and O
SNP O
datasets O
. O

Similar O
to O
the O
toy O
dataset O
from O
Section O
3.4 O
, O
we O
start O
by O
generating O
explanatory O
variables O
for O
both O
genomic O
and O
brain O
imaging O
data O
. O

The O
first O
100 O
components O
of O
are O
drawn O
from O
Normal O
distribution O
, O
while O
the O
rest O
is O
set O
to O
zero O
. O

The O
total O
number O
of O
observations O
is O
set O
to O
n O
= O
200 O
. O

Genomic O
values O
are O
coded O
as O
0 O
( O
no O
minor O
allele O
) O
, O
1 O
( O
one O
minor O
allele O
) O
, O
and O
2 O
( O
two O
minor O
allele O
) O
. O

We O
first O
Define O
a O
minor O
allele O
frequency O
η O
drawn O
from O
a O
uniform O
distribution O
𝒰 O
( O
[ O
0.2 O
, O
0.4 O
] O
) O
. O

The O
i-th O
SNP O
is O
then O
generated O
from O
a O
binomial O
distribution O
ℬ O
( O
2 O
, O
ηi O
) O
. O

For O
the O
imaging O
data O
, O
voxels O
values O
were O
drawn O
from O
a O
Gaussian O
distribution O
𝒩 O
( O
0 O
, O
Ip O
) O
. O

Finally O
, O
binary O
phenotype O
y O
data O
are O
generated O
from O
ℬ O
( O
1 O
, O
di O
) O
, O
where O
. O

Furthermore O
, O
we O
add O
100 O
additional O
variables O
to O
the O
problem O
that O
will O
play O
the O
role O
of O
cross-correlated O
variables O
. O

Two O
canonical O
vectors O
are O
drawn O
from O
Normal O
distribution O
. O

Cross-correlated O
SNP O
are O
drawn O
from O
ℬ O
( O
2 O
, O
logit−1 O
( O
−ai O
+ O
logit O
( O
ηi O
) O
) O
) O
where O
a O
is O
issued O
from O
, O
while O
cross-correlated O
voxels O
are O
drawn O
from O
. O

The O
final O
dataset O
is O
made O
of O
n O
= O
200 O
observations O
of O
p O
= O
1000 O
variables O
for O
both O
SNP O
and O
fMRI O
. O

Each O
of O
these O
datasets O
is O
made O
of O
explanatory O
and O
cross-correlated O
components O
. O

A O
common O
way O
to O
assess O
the O
performance O
of O
a O
model O
when O
it O
comes O
to O
feature O
selection O
is O
to O
measure O
the O
true O
positive O
rate O
( O
TPR O
) O
and O
false O
positive O
rate O
( O
FPR O
) O
. O

TPR O
reflects O
the O
proportion O
of O
variables O
that O
are O
correctly O
identified O
, O
while O
FDR O
reflects O
the O
proportion O
of O
variables O
that O
are O
incorrectly O
selected O
by O
the O
model O
. O

We O
apply O
MT-CoReg O
to O
100 O
random O
generation O
of O
the O
dataset O
described O
above O
. O

The O
tuning O
parameter O
γ O
from O
Eq O
. O

( O
3.3 O
) O
that O
weights O
the O
CCA O
term O
against O
the O
regression O
one O
is O
optimized O
through O
a O
grid O
search O
over O
{ O
[ O
0 O
] O
∪ O
[ O
10−1+ℓ/20 O
] O
, O
ℓ O
= O
0 O
, O
‥ O
, O
20 O
} O
. O

We O
plotted O
TPR O
values O
against O
FDR O
ones O
in O
Fig O
. O

( O
3 O
) O
for O
two O
different O
cases O
. O

In O
the O
first O
( O
left O
) O
subfigure O
are O
displayed O
TPR/FDR O
values O
relative O
to O
non-zero O
components O
of O
for O
γ O
= O
0 O
( O
i.e O
. O

classical O
Lasso O
) O
, O
γ O
= O
γ O
( O
C.S O
. O
) O

where O
the O
weight O
value O
is O
determined O
using O
consistency O
selection O
( O
C.S O
. O
) O

scheme O
described O
in O
Section O
3.3 O
, O
and O
γ O
= O
1 O
( O
i.e O
. O

classical O
sCCA O
) O
. O

We O
can O
observe O
that O
although O
classical O
regression O
seems O
to O
perform O
slightly O
better O
for O
really O
low O
FDR O
values O
, O
MT-CoReg O
is O
quickly O
catching O
up O
around O
FDR O
≈ O
0.15. O
sCCA O
, O
on O
the O
other O
hand O
, O
has O
a O
low O
selection O
power O
. O

The O
second O
( O
bottom O
) O
figure O
displays O
TPR/FDR O
values O
relative O
to O
non-zero O
components O
of O
, O
i.e O
. O

the O
cross-correlated O
components O
. O

We O
can O
observe O
that O
MT-CoReg O
performs O
as O
well O
as O
sCCA O
, O
while O
Lasso O
is O
unable O
to O
properly O
select O
the O
components O
of O
interest O
. O

It O
is O
encouraging O
to O
see O
that O
MT-CoReg O
takes O
the O
best O
of O
both O
methods O
and O
seems O
to O
properly O
select O
the O
components O
we O
are O
interested O
in O
. O

It O
seems O
to O
confirm O
our O
hypothesis O
that O
using O
a O
mix O
of O
both O
terms O
may O
lead O
to O
an O
improved O
feature O
selection O
accuracy O
. O

In O
the O
next O
section O
, O
we O
apply O
the O
same O
method O
to O
a O
real O
dataset O
of O
fMRI O
and O
SNP O
data O
. O

4.2 O
. O

Results O
on O
real O
imaging O
genetics O
data O
4.2.1 O
. O

Data O
acquisition O
Both O
SNP O
and O
fMRI O
acquisition O
were O
conducted O
by O
the O
Mind O
Clinical O
Imaging O
Consortium O
( O
MCIC O
) O
for O
214 O
subjects O
, O
including O
92 O
schizophrenia O
patients O
( O
age O
: O
34 O
± O
11 O
, O
22 O
females O
) O
and O
116 O
controls O
( O
age O
32 O
± O
11 O
, O
44 O
females O
) O
. O

Schizophreniac O
were O
diagnosed O
based O
on O
DSM-IV-TR O
citeria O
. O

Controls O
were O
free O
of O
any O
medical O
, O
neurological O
of O
psychiatric O
illnesses O
. O

fMRI O
were O
acquired O
during O
a O
sensor O
motor O
task O
with O
auditory O
simulation O
. O

Data O
were O
pre-processed O
with O
SPM5 O
, O
spatially O
normalized O
and O
resliced O
, O
smoothed O
, O
and O
analyzed O
by O
multiple O
regression O
considering O
the O
stimulus O
and O
their O
temporal O
derivatives O
plus O
an O
intercept O
term O
as O
regressors O
. O

For O
each O
patient O
, O
a O
stimulus-on O
vs. O
stimulus-off O
contrast O
image O
was O
extracted O
. O

116 O
ROIs O
were O
extracted O
based O
on O
the O
aal O
brain O
atlas O
, O
which O
resulted O
in O
41236 O
voxels O
left O
for O
analysis O
. O

SNP O
data O
were O
obtained O
from O
blood O
sample O
using O
Illumina O
Infinium O
HumanOmni1-Quad O
array O
covering O
1,140,419 O
SNP O
loci O
. O

After O
standard O
quality O
control O
procedures O
using O
PLINK O
software O
packagea O
, O
a O
final O
dataset O
spanning O
777 O
, O
635 O
SNP O
loci O
was O
available O
. O

Each O
SNP O
was O
categorized O
into O
three O
clusters O
based O
on O
their O
genotype O
and O
was O
represented O
with O
discrete O
numbers O
: O
0 O
( O
no O
minor O
allele O
) O
, O
1 O
( O
one O
minor O
allele O
) O
and O
2 O
( O
two O
minor O
alleles O
) O
. O

SNPs O
with O
> O
20 O
% O
missing O
data O
were O
deleted O
and O
missing O
data O
were O
further O
imputed O
. O

SNPs O
with O
minor O
allele O
frequency O
< O
5 O
% O
were O
removed O
. O

This O
procedure O
yielded O
a O
final O
set O
of O
129 O
, O
145 O
SNPs O
. O

4.2.2 O
. O

Significance O
analysis O
In O
order O
to O
achieve O
a O
stable O
feature O
selection O
process O
, O
we O
follow O
Lin O
and O
perform O
N O
= O
100 O
random O
samplings O
out O
of O
the O
214 O
total O
subjects O
, O
where O
for O
each O
time O
80 O
% O
are O
used O
for O
training O
and O
parameter O
selection O
, O
while O
the O
remaining O
20 O
% O
are O
used O
for O
evaluation O
. O

At O
the O
k O
− O
th O
random O
sampling O
, O
we O
can O
calculate O
a O
set O
of O
solution O
vectors O
. O

It O
is O
then O
possible O
to O
Define O
a O
measure O
of O
relevance O
for O
the O
i-th O
feature O
in O
the O
m-th O
dataset O
such O
that O
: O
where O
i O
= O
1 O
, O
‥ O
, O
dm O
is O
the O
feature O
index O
and O
I O
( O
· O
) O
is O
the O
indicator O
function O
. O

We O
can O
then O
rank O
each O
SNP O
and O
voxel O
based O
on O
their O
associated O
relevance O
measure O
and O
apply O
a O
cut-off O
threshold O
of O
0.3 O
( O
c.f O
. O

Lin O
) O
. O

After O
applying O
this O
Significance O
test O
, O
we O
were O
left O
with O
a O
subset O
of O
43 O
SNP O
spanning O
30 O
genes O
and O
6 O
ROI O
with O
a O
number O
of O
selected O
voxels O
over O
5 O
. O

We O
display O
in O
Table O
. O

1 O
the O
list O
of O
each O
of O
the O
43 O
selected O
SNP O
, O
as O
well O
as O
their O
associated O
genes O
. O

Some O
of O
them O
have O
been O
identified O
by O
other O
similar O
studies O
such O
as O
CNTNAP2 O
, O
GLI2 O
, O
GRIK3 O
, O
NOTCH4 O
, O
SUCLG2 O
, O
GABRG2 O
. O

Others O
have O
been O
identified O
from O
well-known O
databases O
such O
as O
GRIK4 O
or O
HTR4 O
. O

We O
display O
in O
Table O
. O

2 O
the O
list O
of O
the O
selected O
ROI O
as O
well O
as O
the O
corresponding O
voxel O
count O
for O
each O
one O
of O
them O
. O

ROI O
for O
which O
less O
than O
5 O
voxels O
were O
selected O
where O
dismissed O
. O

Once O
again O
, O
it O
is O
encouraging O
to O
note O
that O
each O
of O
the O
selected O
ROI O
( O
3 O
, O
7 O
, O
11 O
, O
40 O
, O
51 O
, O
100 O
from O
aal O
. O
) O

have O
been O
identified O
in O
similar O
studies O
on O
the O
same O
dataset O
. O

Other O
studies O
pointed O
out O
both O
functionnal O
or O
structural O
differences O
in O
the O
middle O
occipital O
gyrus O
and O
the O
parahippocampal O
gyrus O
for O
schizophrenic O
patients O
. O

Finally O
, O
a O
detailled O
slice O
view O
of O
the O
selected O
voxels O
can O
be O
seen O
in O
Fig O
. O
( O
4 O
) O
. O

4.2.3 O
. O

Quantitative O
analysis O
In O
this O
section O
, O
we O
try O
to O
analyze O
the O
results O
of O
MT-CoReg O
using O
some O
quantitative O
metrics O
. O

We O
can O
first O
turn O
our O
attention O
to O
the O
Sum O
of O
Squared O
Errors O
( O
SSE O
) O
values O
obtained O
on O
the O
testing O
set O
during O
our O
tests O
. O

Histograms O
of O
SSE O
distributions O
for O
different O
γ O
values O
( O
i.e O
. O

Lasso O
, O
MT-CoReg O
and O
sCCA O
) O
can O
be O
seen O
in O
Fig O
. O

( O
5 O
, O
left O
) O
: O
unsurprisingly O
, O
Lasso O
and O
MT-CoReg O
produce O
the O
lowest O
RSS O
values O
, O
while O
sCCA O
does O
not O
fit O
the O
phenotype O
. O

If O
we O
now O
look O
at O
Fig O
. O

( O
5 O
, O
middle O
) O
where O
distributions O
of O
Pearson O
’ O
s O
correlation O
on O
the O
testing O
set O
are O
displayed O
for O
the O
same O
3 O
strategies O
, O
we O
can O
see O
that O
MT-CoReg O
produces O
a O
better O
selection O
than O
Lasso O
in O
terms O
of O
cross-correlation O
. O

This O
seems O
to O
confirm O
our O
intuition O
that O
MT-CoReg O
makes O
the O
best O
of O
both O
Lasso O
and O
CCA O
by O
producing O
a O
solution O
that O
is O
good O
fit O
to O
the O
phenotype O
while O
selecting O
co-expressed O
features O
across O
modalities O
. O

Distribution O
of O
γ O
values O
produced O
by O
the O
consistency O
selection O
scheme O
described O
in O
Section O
3.3 O
can O
be O
seen O
in O
Fig O
. O

( O
5 O
, O
right O
) O
. O

Most O
of O
these O
values O
fall O
into O
the O
range O
[ O
0 O
; O
0.4 O
] O
, O
with O
a O
peak O
in O
[ O
0.2 O
; O
0.3 O
] O
. O

It O
does O
appear O
, O
at O
least O
in O
term O
of O
feature O
consistency O
selection O
, O
that O
a O
non-zero O
weight O
for O
the O
CCA O
term O
in O
Eq O
. O

( O
3.3 O
) O
leads O
to O
improved O
performances O
. O

