Reading O
the O
( O
functional O
) O
writing O
on O
the O
( O
structural O
) O
wall O
: O
multimodal O
fusion O
of O
brain O
structural O
and O
function O
via O
a O
deep O
neural O
network O
based O
translation O
approach O
reveals O
novel O
impairments O
in O
schizophrenia O
This O
work O
presents O
a O
novel O
approach O
using O
a O
deep O
neural O
network O
for O
finding O
linkage/association O
between O
multimodal O
brain O
imaging O
data O
, O
such O
as O
structural O
MRI O
( O
sMRI O
) O
and O
functional O
MRI O
( O
fMRI O
) O
. O

Motivated O
by O
the O
machine O
translation O
domain O
, O
we O
consider O
two O
different O
imaging O
views O
of O
the O
same O
brain O
like O
two O
different O
languages O
conveying O
some O
common O
facts O
that O
enables O
finding O
linkages O
between O
two O
modalities O
. O

The O
proposed O
translation O
based O
fusion O
model O
contains O
a O
computing O
layer O
that O
learns O
“ O
alignments O
” O
( O
or O
links O
) O
between O
dynamic O
connectivity O
features O
from O
fMRI O
data O
and O
static O
gray O
matter O
patterns O
from O
sMRI O
data O
. O

The O
approach O
is O
evaluated O
on O
a O
multi-site O
dataset O
consisting O
of O
eye-closed O
resting O
state O
imaging O
data O
collected O
from O
298 O
subjects O
( O
age- O
and O
gender O
matched O
154 O
healthy O
controls O
and O
144 O
patients O
with O
schizophrenia O
) O
. O

We O
used O
dynamic O
functional O
connectivity O
( O
dFNC O
) O
states O
as O
the O
functional O
features O
and O
ICA-based O
sources O
from O
gray O
matter O
densities O
as O
the O
structural O
features O
. O

The O
dFNC O
states O
characterized O
by O
weakly O
correlated O
intrinsic O
connectivity O
networks O
( O
ICNs O
) O
were O
found O
to O
have O
stronger O
association O
with O
putamen O
and O
insular O
gray O
matter O
pattern O
, O
while O
the O
dFNC O
states O
of O
profuse O
strongly O
correlated O
ICNs O
exhibited O
stronger O
links O
with O
the O
gray O
matter O
pattern O
in O
precuneus O
, O
posterior O
cingulate O
cortex O
( O
PCC O
) O
, O
and O
temporal O
cortex O
. O

Further O
investigation O
with O
the O
estimated O
link O
strength O
( O
or O
alignment O
score O
) O
showed O
significant O
group O
differences O
between O
healthy O
controls O
and O
patients O
with O
schizophrenia O
in O
several O
key O
regions O
including O
temporal O
lobe O
, O
and O
linked O
these O
to O
connectivity O
states O
showing O
less O
occupancy O
in O
healthy O
controls O
. O

Moreover O
, O
this O
novel O
approach O
revealed O
significant O
correlation O
between O
a O
cognitive O
score O
( O
attention/vigilance O
) O
and O
the O
function/structure O
alignment O
score O
that O
was O
not O
detected O
when O
data O
modalities O
were O
considered O
separately O
. O

Materials O
and O
Methods O
We O
first O
briefly O
describe O
about O
data O
collection O
and O
preprocessing O
, O
and O
then O
the O
translation-based O
fusion O
model O
is O
explained O
in O
this O
section O
. O

Participants O
In O
this O
work O
, O
we O
perform O
analysis O
on O
two O
modalities O
of O
data O
, O
T1-weighted O
structural O
images O
and O
T2 O
* O
-weighted O
functional O
images O
. O

The O
resting O
state O
fMRI O
data O
were O
collected O
from O
154 O
healthy O
controls O
( O
110 O
males O
, O
44 O
females O
; O
mean O
age O
37 O
) O
and O
144 O
schizophrenic O
patients O
( O
110 O
males O
, O
34 O
females O
; O
mean O
age O
38 O
) O
during O
eye O
closed O
condition O
at O
seven O
different O
scanning O
sites O
. O

A O
total O
of O
162 O
volumes O
of O
echo O
planar O
imaging O
BOLD O
fMRI O
data O
were O
collected O
with O
a O
TR O
of O
2 O
s O
on O
3T O
scanners O
. O

For O
the O
same O
subjects O
, O
T1-weighted O
structural O
images O
were O
collected O
as O
well O
. O

Full O
details O
on O
the O
participants O
and O
data O
collection O
can O
be O
found O
in O
and O
a O
summary O
of O
demographics O
are O
provided O
in O
. O

Data O
collection O
MR O
images O
were O
collected O
on O
a O
3-Tesla O
Siemens O
Trio O
scanner O
at O
six O
sites O
and O
on O
a O
3T O
General O
Electric O
Discovery O
MR750 O
scanner O
at O
one O
site O
. O

High-resolution O
T1-weighted O
structural O
images O
were O
acquired O
with O
a O
turbo-flash O
sequence O
( O
TE O
= O
2.94 O
ms O
, O
TR O
= O
2.3 O
s O
, O
flip O
angle O
= O
9° O
, O
number O
of O
excitations O
= O
1 O
, O
slice O
thickness O
= O
1.2 O
mm O
, O
field O
of O
view O
= O
256 O
mm O
, O
resolution O
= O
256 O
× O
256 O
) O
resulting O
in O
0.86 O
× O
0.86 O
× O
1.2 O
mm3 O
voxels O
. O

T2 O
* O
-weighted O
functional O
images O
were O
acquired O
using O
a O
gradient-echo O
EPI O
sequence O
( O
TR/TE O
2s/30ms O
, O
flip O
angle O
77 O
degrees O
, O
32 O
slices O
collected O
sequentially O
from O
superior O
to O
inferior O
, O
3.4 O
× O
3.4 O
× O
4 O
mm3 O
with O
1 O
mm O
gap O
, O
162 O
frames O
, O
5:38 O
min O
) O
. O

Participants O
were O
instructed O
to O
keep O
their O
eyes O
closed O
during O
the O
scan O
. O

Data O
preprocessing O
Structural O
data O
: O
T1-weighted O
images O
were O
normalized O
to O
MNI O
space O
, O
resliced O
to O
2 O
× O
2 O
× O
2 O
mm O
, O
and O
segmented O
into O
gray O
, O
white O
, O
and O
CSF O
images O
using O
the O
unified O
segmentation O
methods O
of O
SPM5 O
. O

Data O
quality O
was O
checked O
by O
correlations O
against O
the O
segmented O
templates O
; O
if O
the O
subject O
’ O
s O
segmented O
gray O
matter O
data O
did O
not O
correlate O
at O
0.9 O
or O
higher O
with O
the O
template O
across O
all O
voxels O
, O
it O
was O
removed O
from O
consideration O
. O

Gray O
matter O
segmentations O
were O
finally O
smoothed O
by O
a O
Gaussian O
filter O
of O
10 O
mm O
Full O
Width O
Half O
Maximum O
( O
FWHM O
) O
. O

We O
analyzed O
gray O
matter O
density O
( O
GMD O
) O
with O
independent O
component O
analysis O
( O
ICA O
) O
to O
extract O
features O
as O
relationships O
among O
GMD O
regions O
, O
which O
is O
called O
source-based O
morphometry O
( O
SBM O
) O
. O

The O
ICA O
was O
performed O
using O
the O
group O
ICA O
of O
fMRI O
( O
GIFT O
) O
toolbox1 O
and O
50 O
components O
were O
estimated O
. O

After O
analyzing O
stability O
of O
the O
components O
and O
visual O
inspection O
, O
23 O
components O
were O
selected O
, O
and O
hereafter O
they O
are O
referred O
to O
as O
structural O
components O
. O

Functional O
data O
: O
We O
used O
resting O
state O
fMRI O
data O
and O
performed O
rigid O
body O
motion O
correction O
using O
the O
INRIAlign O
toolbox O
in O
SPM O
to O
correct O
for O
subject O
head O
motion O
followed O
by O
slice-timing O
correction O
to O
account O
for O
timing O
differences O
in O
slice O
acquisition O
. O

Then O
the O
fMRI O
data O
were O
despiked O
using O
AFNI O
’ O
s O
3dDespike O
algorithm O
to O
mitigate O
the O
impact O
of O
outliers O
. O

The O
fMRI O
data O
were O
subsequently O
warped O
to O
a O
Montreal O
Neurological O
Institute O
( O
MNI O
) O
template O
and O
resampled O
to O
3 O
mm3 O
isotropic O
voxels O
. O

Instead O
of O
Gaussian O
smoothing O
, O
we O
smoothed O
the O
data O
to O
6 O
mm O
full O
width O
at O
half O
maximum O
( O
FWHM O
) O
using O
AFNI O
’ O
s O
BlurToFWHM O
algorithm O
which O
performs O
smoothing O
by O
a O
conservative O
finite O
difference O
approximation O
to O
the O
diffusion O
equation O
. O

This O
approach O
has O
been O
shown O
to O
reduce O
scanner O
specific O
variability O
in O
smoothness O
providing O
“ O
smoothness O
equivalence O
” O
to O
data O
across O
sites O
. O

Each O
voxel O
time O
course O
was O
variance O
normalized O
prior O
to O
performing O
group O
independent O
component O
analysis O
. O

These O
processed O
data O
were O
then O
decomposed O
into O
components O
using O
spatial O
group O
independent O
component O
analysis O
( O
GICA O
) O
implemented O
in O
the O
GIFT O
toolbox O
. O

Each O
component O
can O
be O
regarded O
as O
temporally O
coherent O
intrinsic O
connectivity O
networks O
( O
ICN O
) O
, O
and O
47 O
such O
networks O
were O
selected O
as O
in O
. O

For O
feature O
representation O
, O
pairwise O
correlation O
between O
ICN O
time O
courses O
were O
computed O
yielding O
a O
correlation O
matrix O
of O
size O
47 O
× O
47 O
. O

In O
order O
to O
capture O
dynamics O
, O
correlation O
was O
estimated O
using O
a O
sliding O
window O
approach O
with O
a O
window O
size O
of O
22 O
TR O
( O
44 O
s O
) O
in O
steps O
of O
1 O
TR O
( O
2 O
s O
) O
[ O
see O
for O
details O
] O
. O

We O
refer O
to O
this O
windowed O
correlation O
matrix O
as O
dynamic O
functional O
network O
connectivity O
( O
dFNC O
) O
. O

However O
, O
in O
order O
to O
reduce O
the O
total O
time O
steps O
for O
our O
translation O
model O
, O
we O
took O
average O
of O
every O
4 O
consecutive O
correlation O
matrices O
. O

Finally O
, O
a O
discrete O
sequence O
of O
dFNC O
states O
were O
obtained O
using O
k-means O
clustering O
algorithm O
on O
the O
dFNC O
matrices O
, O
with O
a O
setting O
of O
k O
= O
5 O
using O
the O
elbow O
criterion O
. O

Translation-based O
multimodal O
fusion O
model O
Machine O
translation O
models O
that O
produce O
sentences O
in O
one O
language O
from O
another O
are O
common O
in O
the O
natural O
language O
processing O
discipline O
. O

Essentially O
, O
different O
languages O
convey O
common O
concept O
or O
fact O
in O
different O
ways O
with O
their O
own O
constructs O
. O

We O
can O
consider O
sMRI O
and O
fMRI O
as O
two O
different O
views O
of O
the O
same O
brain O
, O
and O
take O
an O
approach O
from O
the O
machine O
translation O
discipline O
to O
deal O
with O
multimodal O
neuroimaging O
data O
. O

A O
recently O
proposed O
neural O
machine O
translation O
model O
has O
shown O
state-of-the-art O
performance O
with O
its O
novel O
attention O
mechanism O
. O

The O
main O
feature O
of O
the O
attention O
module O
is O
to O
learn O
alignment O
between O
phrases O
of O
two O
different O
languages O
for O
improving O
translation O
performance O
. O

We O
exploit O
this O
idea O
of O
attention O
mechanism O
to O
learn O
alignment O
( O
linkage O
) O
between O
dFNC O
states O
and O
brain O
structural O
components O
. O

However O
, O
unlike O
the O
sequence O
to O
sequence O
matching O
in O
the O
language O
translation O
, O
input O
in O
our O
case O
is O
an O
unordered O
set O
of O
sMRI O
component O
loadings O
and O
output O
are O
temporally O
ordered O
dFNC O
states O
. O

To O
tackle O
this O
problem O
, O
we O
propose O
a O
simple O
modification O
in O
the O
attention O
network O
in O
our O
translation O
model O
. O

Figure O
1 O
depicts O
the O
different O
parts O
of O
our O
translation O
model O
in O
the O
context O
of O
neuroimaging O
. O

The O
model O
stacks O
several O
neural-network O
layers O
( O
six O
layers O
in O
total O
) O
which O
we O
describe O
below O
. O

As O
shown O
in O
Fig O
. O

1 O
, O
two O
main O
parts O
of O
our O
translation-based O
fusion O
model O
are O
: O
( O
1 O
) O
sequence O
predictor O
and O
( O
2 O
) O
attention O
network O
. O

The O
input-output O
setting O
of O
the O
model O
is O
as O
follows O
. O

Input O
is O
an O
unordered O
set O
of O
structural O
component O
loadings O
of O
a O
subject O
, O
x O
= O
{ O
x1 O
, O
… O
, O
xj O
, O
… O
xJ O
} O
, O
and O
the O
output O
is O
a O
temporally O
ordered O
dFNC O
state O
sequence O
, O
y O
= O
{ O
y1 O
, O
… O
, O
yi O
, O
…yT O
} O
, O
of O
the O
same O
subject O
estimated O
from O
the O
preprocessing O
step O
described O
in O
Section O
2.3 O
. O

The O
central O
theme O
of O
the O
model O
is O
that O
information O
for O
predicting O
a O
sequence O
y O
from O
the O
corresponding O
loading O
coefficients O
x O
may O
spread O
through O
out O
the O
structural O
components O
, O
which O
can O
be O
selectively O
retrieved O
as O
the O
sequence O
predictor O
predicts O
a O
dFNC O
state O
at O
each O
time O
step O
. O

This O
is O
achieved O
by O
training O
both O
sequence O
predictor O
and O
attention O
network O
jointly O
from O
the O
multimodal O
data O
. O

Further O
details O
of O
the O
model O
are O
described O
below O
. O

Sequence O
predictor O
The O
sequence O
predictor O
is O
a O
probabilistic O
model O
that O
predicts O
one O
dFNC O
state O
of O
a O
sequence O
at O
each O
time O
step O
, O
where O
we O
define O
each O
conditional O
probability O
as O
where O
si O
is O
the O
current O
hidden O
state O
of O
a O
unidirectional O
recurrent O
layer O
and O
ci O
( O
more O
details O
will O
be O
provided O
shortly O
) O
is O
the O
current O
selective O
focus O
over O
structural O
components O
, O
referred O
to O
as O
context O
hereafter O
. O

A O
few O
points O
about O
the O
probability O
model O
of O
Eq O
. O

( O
1 O
) O
are O
noteworthy O
. O

First O
, O
it O
embodies O
a O
fusion O
implicitly O
as O
the O
probability O
is O
conditioned O
on O
previous O
output O
history O
( O
from O
one O
modality O
) O
and O
the O
input O
( O
from O
the O
other O
modality O
) O
. O

Second O
, O
the O
time O
index O
i O
indicates O
dynamic O
property O
of O
one O
of O
data O
modalities O
. O

Finally O
, O
right O
hand O
side O
of O
Eq O
. O

( O
1 O
) O
captures O
the O
aspect O
of O
deep O
learning O
, O
i.e. O
, O
the O
predictor O
works O
with O
latent O
representations O
of O
input O
and O
output O
as O
opposed O
to O
the O
direct O
input-output O
, O
which O
are O
learned O
from O
the O
data O
. O

We O
realize O
Eq O
. O

( O
1 O
) O
by O
a O
feedforward O
neural O
network O
( O
NN O
) O
[ O
a O
single O
hidden O
layer O
with O
softmax O
output O
] O
stacking O
it O
on O
a O
recurrent O
layer O
. O

At O
each O
time O
point O
, O
the O
recurrent O
layer O
computes O
the O
current O
hidden O
state O
si O
which O
is O
a O
function O
of O
past O
state O
, O
previous O
output O
from O
the O
feedforward O
NN O
, O
and O
the O
current O
context O
, O
i.e. O
, O
The O
recurrent O
layer O
helps O
finding O
a O
learnable O
smooth O
trajectory O
in O
a O
latent O
representational O
space O
. O

We O
use O
gated O
recurrent O
units O
( O
GRUs O
) O
in O
the O
recurrent O
layer O
as O
they O
work O
well O
for O
sequence O
representation O
. O

Each O
output O
dFNC O
state O
yi O
indicates O
one O
of O
the O
centroids O
of O
five O
clusters O
( O
see O
Section O
2.3 O
) O
. O

Since O
the O
centroids O
are O
47 O
× O
47 O
matrices O
lying O
in O
rather O
a O
low O
dimensional O
subspace O
, O
we O
reduce O
the O
dimension O
into O
4 O
, O
i.e. O
, O
yi O
∈ O
R4 O
, O
using O
principal O
component O
analysis O
( O
PCA O
) O
. O

The O
remaining O
term O
, O
current O
context O
ci O
, O
is O
described O
in O
the O
next O
subsection O
. O

Attention O
network O
For O
our O
study O
, O
attention O
network O
is O
the O
most O
important O
part O
as O
it O
enables O
learning O
association O
( O
s O
) O
between O
functional O
dynamics O
and O
structural O
features O
. O

Just O
before O
the O
sequence O
predictor O
predicts O
i-th O
dFNC O
state O
, O
the O
attention O
network O
first O
computes O
an O
alignment O
score O
( O
indicating O
strength O
of O
association O
) O
as O
to O
how O
well O
the O
structural O
component O
xj O
matches O
with O
dFNC O
state O
yi O
. O

This O
score O
is O
based O
on O
recurrent O
state O
si–1 O
and O
evaluated O
for O
all O
structural O
components O
, O
i.e. O
, O
for O
j O
= O
1 O
, O
2 O
, O
… O
, O
J O
, O
in O
each O
time O
step O
i O
. O

We O
use O
a O
feedforward O
neural O
network O
( O
NN O
) O
with O
a O
single O
hidden O
layer O
for O
the O
attention O
module O
as O
described O
below O
. O

Here O
V O
, O
Ws O
and O
Wx O
are O
the O
parameters O
of O
a O
feedforward O
NN O
, O
and O
ei O
is O
a O
vector O
of O
length O
J O
containing O
unnormalized O
alignments O
. O

Then O
the O
normalized O
alignments O
are O
computed O
according O
to O
Eq O
. O

( O
3 O
) O
to O
provide O
a O
probabilistic O
interpretation O
. O

The O
attention O
network O
modulates O
the O
structural O
components O
with O
its O
learned O
alignments O
and O
computes O
a O
context O
vector O
ci O
at O
i-th O
time O
step O
as O
where O
● O
indicates O
element O
wise O
multiplication O
. O

In O
other O
words O
, O
the O
context O
vector O
serves O
as O
the O
currently O
focused O
structural O
components O
with O
their O
soft O
alignments O
. O

In O
effect O
, O
each O
alignment O
αij O
reflects O
the O
importance O
of O
structural O
component O
xj O
with O
respect O
to O
previous O
hidden O
state O
si–1 O
in O
deciding O
next O
state O
si O
and O
generating O
dFNC O
state O
yi O
by O
the O
sequence O
predictor O
. O

Our O
interest O
with O
the O
translation-based O
fusion O
model O
described O
above O
is O
to O
examine O
brain O
structure-function O
relationship O
in O
terms O
of O
the O
alignments O
, O
αij O
, O
for O
i-th O
dFNC O
state O
and O
j-th O
structural O
components O
. O

Note O
that O
these O
alignments O
are O
learned O
from O
the O
data O
, O
thus O
taking O
representational O
advantage O
of O
deep O
learning O
. O

Since O
similar O
models O
have O
been O
proved O
to O
be O
able O
to O
find O
meaningful O
associations O
in O
machine O
translation O
( O
e.g. O
, O
alignment O
between O
phrases O
of O
two O
languages O
) O
and O
image O
caption O
generation O
( O
e.g. O
, O
association O
between O
phrases O
in O
text O
and O
regions O
in O
image O
) O
, O
we O
rely O
on O
our O
method O
for O
finding O
associations O
between O
fMRI O
and O
sMRI O
features O
faithfully O
. O

Both O
the O
sequence O
predictor O
and O
attention O
network O
are O
trained O
jointly O
using O
a O
gradient O
based O
optimization O
algorithm O
called O
rmsprop O
with O
respect O
to O
a O
negative O
log-likelihood O
based O
cost O
function O
, O
In O
order O
to O
avoid O
the O
overfitting O
problem O
, O
we O
use O
L2 O
regularization O
on O
alignments O
and O
a O
50 O
% O
dropout O
in O
the O
hidden O
layers O
of O
feedforward O
NNs O
( O
see O
Fig O
. O

1 O
) O
. O

No O
dropout O
was O
adopted O
in O
the O
recurrent O
layer O
and O
inputs O
. O

There O
are O
some O
architectural O
choices O
and O
hyper O
parameters O
for O
our O
model O
shown O
in O
Fig O
. O

1 O
. O

Based O
on O
the O
lowest O
negative O
log-likelihood O
on O
a O
hold O
out O
subset O
of O
data O
over O
several O
configurations O
, O
we O
selected O
number O
of O
the O
hidden O
neurons O
in O
both O
feedforward O
NNs O
as O
50 O
, O
the O
number O
of O
recurrent O
units O
in O
the O
recurrent O
layer O
as O
50 O
; O
and O
set O
the O
learning O
rate O
and O
the O
coefficient O
of O
L2 O
norm O
as O
0.01 O
and O
0.5 O
, O
respectively O
. O

The O
model O
with O
this O
configuration O
was O
trained O
using O
a O
gradient O
descent O
algorithm O
over O
entire O
data O
, O
and O
then O
the O
alignments O
were O
extracted O
from O
the O
model O
. O

We O
took O
100 O
runs O
with O
different O
random O
neural-network O
weight O
initializations O
of O
the O
model O
, O
and O
the O
alignments O
were O
averaged O
over O
100 O
runs O
for O
subsequent O
analysis O
. O

